{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KWW8xvrp5TdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3cb088-6c0b-4a5c-f684-c8743165654b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size train set: (7165, 5) \n",
            "Size test set: (4, 3)\n",
            "Size train promts: (4, 4) \n",
            "Size test promts: (2, 4)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import zipfile\n",
        "\n",
        "train_df_promt = pd.read_csv('prompts_train.csv')\n",
        "train_df = pd.read_csv('summaries_train.csv')\n",
        "test_df_promt = pd.read_csv('prompts_test.csv')\n",
        "test_df = pd.read_csv('summaries_test.csv')\n",
        "print(f'Size train set: {train_df.shape} \\nSize test set: {test_df.shape}')\n",
        "print(f'Size train promts: {train_df_promt.shape} \\nSize test promts: {test_df_promt.shape}')\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_text(text, max_length=40):\n",
        "    \"\"\"Truncates the string to the specified number of characters\"\"\"\n",
        "    if len(text) > max_length:\n",
        "        return text[:max_length] + '...'\n",
        "    return text\n",
        "\n",
        "train_sample_promt = train_df_promt.head(4).copy()\n",
        "test_sample_promt = test_df_promt.head(2).copy()\n",
        "train_sample = train_df.head(4).copy()\n",
        "test_sample = test_df.head(2).copy()\n",
        "\n",
        "train_sample_promt.loc[:, 'prompt_text'] = train_sample_promt['prompt_text'].apply(truncate_text)\n",
        "train_sample_promt.loc[:, 'prompt_question'] = train_sample_promt['prompt_question'].apply(truncate_text)\n",
        "test_sample_promt.loc[:, 'prompt_text'] = test_sample_promt['prompt_text'].apply(truncate_text)\n",
        "train_sample.loc[:, 'text'] = train_sample['text'].apply(truncate_text)"
      ],
      "metadata": {
        "id": "FbA7P3D15en4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = train_df.merge(train_df_promt, on='prompt_id')"
      ],
      "metadata": {
        "id": "XGn57Q3t5jJa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def clean_text(text):\n",
        "    # Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "    # Удаление знаков препинания, специальных символов и чисел\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Удаление лишних пробелов\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "stop_words = {\n",
        "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\",\n",
        "    \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\",\n",
        "    \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\",\n",
        "    \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n",
        "    \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
        "    \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\",\n",
        "    \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\",\n",
        "    \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\",\n",
        "    \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\",\n",
        "    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\",\n",
        "    \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\",\n",
        "    \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\",\n",
        "    \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\",\n",
        "    \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
        "}\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# def lemmatize_text(tokens):\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "#     lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "#     return lemmatized_tokens"
      ],
      "metadata": {
        "id": "DKyBXYI05pTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab4b6d0b-79a5-47fc-ec65-0bbd80b6ff84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['text'] = df_train['text'].apply(clean_text)\n",
        "df_train['text'] = df_train['text'].apply(tokenize_text)\n",
        "df_train['text'] = df_train['text'].apply(remove_stopwords)\n",
        "# df_train['text'] = df_train['text'].apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "OF4w_7Ov56Bw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_train['text']  # Признаки: текст\n",
        "y_content = df_train['content']  # Целевая переменная 1: content\n",
        "y_wording = df_train['wording']  # Целевая переменная 2: wording\n",
        "\n",
        "X_train, X_test, y_train_content, y_test_content, y_train_wording, y_test_wording = train_test_split(\n",
        "    X, y_content, y_wording, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "OBJ47j7q60sg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "# Если X_train содержит списки токенов, нужно объединить их обратно в строки\n",
        "X_train = [' '.join(text) if isinstance(text, list) else text for text in X_train]\n",
        "X_test = [' '.join(text) if isinstance(text, list) else text for text in X_test]\n",
        "\n",
        "# Теперь можно преобразовать данные в TF-IDF\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "XuTQpWLg67Q1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf[:3]"
      ],
      "metadata": {
        "id": "v1aG7ORpCRcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df32fe9e-c350-44a1-89f9-9622f14a85f8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 93 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Предположим, что X_train и X_test уже определены ранее\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Если X_train содержит списки токенов, нужно объединить их обратно в строки\n",
        "X_train = [' '.join(text) if isinstance(text, list) else text for text in X_train]\n",
        "X_test = [' '.join(text) if isinstance(text, list) else text for text in X_test]\n",
        "\n",
        "# Преобразование данных в TF-IDF\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Преобразование разреженной матрицы в плотный формат\n",
        "X_train_dense = X_train_tfidf.toarray()\n",
        "X_test_dense = X_test_tfidf.toarray()\n",
        "\n",
        "# Масштабирование целей (если необходимо)\n",
        "scaler = StandardScaler()\n",
        "y_train_content = scaler.fit_transform(y_train_content.values.reshape(-1, 1)).flatten()\n",
        "y_test_content = scaler.transform(y_test_content.values.reshape(-1, 1)).flatten()\n",
        "y_train_wording = scaler.fit_transform(y_train_wording.values.reshape(-1, 1)).flatten()\n",
        "y_test_wording = scaler.transform(y_test_wording.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Создание модели нейронной сети\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_dim=X_train_dense.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1))  # Для предсказания одного значения (content или wording)\n",
        "\n",
        "# Компиляция модели\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "\n",
        "# Раннее завершение обучения при отсутствии улучшений\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Обучение модели для метрики 'content'\n",
        "history_content = model.fit(X_train_dense, y_train_content, validation_split=0.2, epochs=100, batch_size=32, callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "# Обучение модели для метрики 'wording'\n",
        "history_wording = model.fit(X_train_dense, y_train_wording, validation_split=0.2, epochs=100, batch_size=32, callbacks=[early_stopping], verbose=1)\n"
      ],
      "metadata": {
        "id": "hcN0oz1bUpoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b9765f9-c69a-4404-ff66-fa982c0306a4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 72ms/step - loss: 0.7719 - mean_squared_error: 0.7719 - val_loss: 0.3541 - val_mean_squared_error: 0.3541\n",
            "Epoch 2/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - loss: 0.2364 - mean_squared_error: 0.2364 - val_loss: 0.3367 - val_mean_squared_error: 0.3367\n",
            "Epoch 3/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 52ms/step - loss: 0.1330 - mean_squared_error: 0.1330 - val_loss: 0.3371 - val_mean_squared_error: 0.3371\n",
            "Epoch 4/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 43ms/step - loss: 0.0867 - mean_squared_error: 0.0867 - val_loss: 0.3261 - val_mean_squared_error: 0.3261\n",
            "Epoch 5/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 50ms/step - loss: 0.0705 - mean_squared_error: 0.0705 - val_loss: 0.3203 - val_mean_squared_error: 0.3203\n",
            "Epoch 6/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 52ms/step - loss: 0.0653 - mean_squared_error: 0.0653 - val_loss: 0.3332 - val_mean_squared_error: 0.3332\n",
            "Epoch 7/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - loss: 0.0579 - mean_squared_error: 0.0579 - val_loss: 0.3308 - val_mean_squared_error: 0.3308\n",
            "Epoch 8/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 50ms/step - loss: 0.0567 - mean_squared_error: 0.0567 - val_loss: 0.3191 - val_mean_squared_error: 0.3191\n",
            "Epoch 9/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - loss: 0.0513 - mean_squared_error: 0.0513 - val_loss: 0.3220 - val_mean_squared_error: 0.3220\n",
            "Epoch 10/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - loss: 0.0487 - mean_squared_error: 0.0487 - val_loss: 0.3154 - val_mean_squared_error: 0.3154\n",
            "Epoch 11/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.3296 - val_mean_squared_error: 0.3296\n",
            "Epoch 12/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - loss: 0.0388 - mean_squared_error: 0.0388 - val_loss: 0.3231 - val_mean_squared_error: 0.3231\n",
            "Epoch 13/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 0.0397 - mean_squared_error: 0.0397 - val_loss: 0.3298 - val_mean_squared_error: 0.3298\n",
            "Epoch 14/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 53ms/step - loss: 0.0391 - mean_squared_error: 0.0391 - val_loss: 0.3273 - val_mean_squared_error: 0.3273\n",
            "Epoch 15/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 78ms/step - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.3195 - val_mean_squared_error: 0.3195\n",
            "Epoch 1/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 53ms/step - loss: 0.3776 - mean_squared_error: 0.3776 - val_loss: 0.4839 - val_mean_squared_error: 0.4839\n",
            "Epoch 2/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - loss: 0.1400 - mean_squared_error: 0.1400 - val_loss: 0.4748 - val_mean_squared_error: 0.4748\n",
            "Epoch 3/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - loss: 0.0795 - mean_squared_error: 0.0795 - val_loss: 0.4906 - val_mean_squared_error: 0.4906\n",
            "Epoch 4/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 53ms/step - loss: 0.0573 - mean_squared_error: 0.0573 - val_loss: 0.5030 - val_mean_squared_error: 0.5030\n",
            "Epoch 5/100\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 44ms/step - loss: 0.0436 - mean_squared_error: 0.0436 - val_loss: 0.4927 - val_mean_squared_error: 0.4927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content_pred = model.predict(X_test_dense)  # Здесь мы используем 'model', а не 'history_content'\n",
        "wording_pred = model.predict(X_test_dense)\n",
        "\n",
        "def calculate_rmse(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return rmse\n",
        "\n",
        "\n",
        "mcrmse_score_content = calculate_rmse(y_test_content, content_pred)\n",
        "mcrmse_score_wording = calculate_rmse(y_test_wording, wording_pred)\n",
        "\n",
        "print(f\"MCRMSE = {np.mean([mcrmse_score_content, mcrmse_score_wording])}\")"
      ],
      "metadata": {
        "id": "0X0Ctn6swxgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "befa2edd-b4e1-4deb-b8b6-9e8ff09e47f4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "MCRMSE = 0.7441264251132851\n"
          ]
        }
      ]
    }
  ]
}